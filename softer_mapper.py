# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mxn1MwhRVjO9NyXpYu6bR7j8YGNuUYB8
"""

!pip install kagglehub

!pip install folium scikit-fuzzy kmapper

!pip install gudhi
import os
import json
import pandas as pd
import numpy as np
import folium
from folium.plugins import HeatMap
import matplotlib.pyplot as plt
import skfuzzy as fuzz
import kmapper as km
from kmapper import Cover
from sklearn.base import BaseEstimator, ClusterMixin
from sklearn.cluster import KMeans, DBSCAN
import gudhi as gd
from google.colab import drive
from folium.plugins import HeatMap
import skfuzzy as fuzz
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import NearestNeighbors

from google.colab import drive
drive.mount('/content/drive')

!ls /content/drive/MyDrive/text_coordinates_regions

folder_path = "/content/drive/MyDrive/text_coordinates_regions"


# Function to load geo-tagged data
def load_coordinates(folder_path, sample_frac=0.1):
    all_rows = []
    for filename in os.listdir(folder_path):
        if filename.endswith(".json"):
            file_path = os.path.join(folder_path, filename)
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        try:
                            entry = json.loads(line.strip())
                            coords = entry.get("coordinates")
                            if coords and isinstance(coords, list) and len(coords) == 2:
                                lon = float(coords[0])
                                lat = float(coords[1])
                                if -90 <= lat <= 90 and -180 <= lon <= 180:  # Validate coordinates
                                    all_rows.append({"longitude": lon, "latitude": lat})
                        except Exception as e:
                            print(f"Error parsing line in {filename}: {e}")
            except Exception as e:
                print(f"Error opening file {filename}: {e}")

    df = pd.DataFrame(all_rows)
    # Sample to reduce memory usage for large datasets
    if len(df) > 10000:
        df = df.sample(frac=sample_frac, random_state=42)
    return df

# Load and preprocess data
df = load_coordinates(folder_path)
print("DataFrame shape:", df.shape)
df.head()

df_vis = df.sample(n=2000) if len(df) > 2000 else df.copy()

map_center = [df_vis["latitude"].median(), df_vis["longitude"].median()]

base_map = folium.Map(location=map_center, zoom_start=2, tiles='CartoDB positron')
for _, row in df_vis.iterrows():
    folium.CircleMarker(
        location=[row['latitude'], row['longitude']],
        radius=1,
        color='blue',
        fill=True,
        fill_opacity=0.6
    ).add_to(base_map)
base_map

heat_map = folium.Map(location=map_center, zoom_start=2, tiles='CartoDB positron')
heat_data = [[row['latitude'], row['longitude']] for _, row in df_vis.iterrows()]
HeatMap(heat_data, radius=6, blur=5).add_to(heat_map)
heat_map

# Preprocess data
def preprocess_data(X):
    mask = (X[:, 0] >= -90) & (X[:, 0] <= 90) & (X[:, 1] >= -180) & (X[:, 1] <= 180)
    X = X[mask]
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    return X_scaled, mask

X, mask = preprocess_data(df[['latitude', 'longitude']].values)
df_filtered = df[mask].reset_index(drop=True)
print("Preprocessed data shape:", X.shape)

# Custom FuzzyClusterer
class FuzzyClusterer(BaseEstimator, ClusterMixin):
    def __init__(self, n_clusters=2, m=2, max_iter=300, error=1e-5):
        self.n_clusters = n_clusters
        self.m = m
        self.max_iter = max_iter
        self.error = error

    def fit(self, X, y=None):
        data = X.T
        try:
            cntr, u, _, _, _, _, _ = fuzz.cluster.cmeans(
                data, c=self.n_clusters, m=self.m, error=self.error, maxiter=self.max_iter, init=None
            )
            self.cluster_centers_ = cntr
            self.membership_ = u
        except Exception as e:
            print(f"FCM failed: {e}")
            self.cluster_centers_ = X[np.random.choice(X.shape[0], self.n_clusters, replace=False)]
            self.membership_ = np.ones((self.n_clusters, X.shape[0])) / self.n_clusters
        return self

    def fit_predict(self, X, y=None):
        n_samples = X.shape[0]
        if n_samples == 0:
            return np.array([])
        if n_samples < self.n_clusters:
            return np.zeros(n_samples, dtype=int)

        self.fit(X)
        labels = np.argmax(self.membership_, axis=0)
        return labels

# Enhanced Persistent Homology-based Filter
def persistence_filter(X, n_simplices=1000, max_edge_length=2.0):
    # Subsample
    if X.shape[0] > n_simplices:
        indices = np.random.choice(X.shape[0], n_simplices, replace=False)
        X_sample = X[indices]
    else:
        X_sample = X
        indices = np.arange(X.shape[0])

    # Remove duplicates
    X_sample_unique, unique_indices = np.unique(X_sample, axis=0, return_index=True)
    if len(X_sample_unique) < 2:
        print("Too few unique points, using l2norm")
        lens = np.linalg.norm(X, axis=1) / (np.linalg.norm(X, axis=1).max() + 1e-10)
        return lens.reshape(-1, 1)

    # Compute Rips complex on unique points
    rips_complex = gd.RipsComplex(points=X_sample_unique, max_edge_length=max_edge_length)
    simplex_tree = rips_complex.create_simplex_tree(max_dimension=1)
    persistence = simplex_tree.persistence()

    # Extract H0 persistence
    h0_persistence = []
    for p in persistence:
        if p[0] == 0:
            diff = p[1][1] - p[1][0]
            if np.isfinite(diff) and diff > 0:
                h0_persistence.append(diff)

    # Initialize lens
    lens = np.zeros(X.shape[0])

    if len(h0_persistence) > 0:
        # Pad or truncate h0_persistence to match X_sample_unique length
        h0_array = np.array(h0_persistence)
        if len(h0_array) < len(X_sample_unique):
            h0_array = np.pad(h0_array, (0, len(X_sample_unique) - len(h0_array)), mode='constant', constant_values=0)
        else:
            h0_array = h0_array[:len(X_sample_unique)]

        # Compute density weighting
        nn = NearestNeighbors(n_neighbors=1).fit(X_sample_unique)
        distances, _ = nn.kneighbors(X_sample_unique)
        density_weight = 1 - distances[:, 0] / (distances[:, 0].max() + 1e-10)

        # Map back to original X_sample indices
        lens_values = h0_array * density_weight
        lens[indices[unique_indices]] = lens_values
    else:
        # Fallback: nearest-neighbor distances
        nn = NearestNeighbors(n_neighbors=2).fit(X_sample_unique)
        distances, _ = nn.kneighbors(X_sample_unique)
        lens[indices[unique_indices]] = distances[:, 1] / (distances[:, 1].max() + 1e-10)

    # Replace inf/nan
    lens = np.where(np.isfinite(lens), lens, 0.0)

    # Ensure variation
    if lens.std() < 1e-5 or lens.max() == lens.min():
        print("Lens too uniform, using l2norm")
        lens = np.linalg.norm(X, axis=1) / (np.linalg.norm(X, axis=1).max() + 1e-10)

    print("Lens stats - min:", lens.min(), "max:", lens.max(), "std:", lens.std())
    return lens.reshape(-1, 1)

# Soft Mapper Implementation
mapper = km.KeplerMapper(verbose=2)

# Compute lens
lens = persistence_filter(X, n_simplices=1000, max_edge_length=2.0)

# Debug lens
print("Lens shape:", lens.shape)
print("Unique lens values:", np.unique(lens).shape[0])

# Check for degenerate lens
if lens.max() == lens.min() or np.any(~np.isfinite(lens)):
    print("Warning: Degenerate lens, falling back to l2norm")
    lens = mapper.fit_transform(X, projection="l2norm")

# Configure cover
cover = Cover(n_cubes=5, perc_overlap=0.2)

# Initialize FuzzyClusterer
fuzzy_clusterer = FuzzyClusterer(n_clusters=2)

# Build Mapper complex
try:
    complex_res = mapper.map(lens, X, cover=cover, clusterer=fuzzy_clusterer)
    print("Number of nodes in complex:", len(complex_res["nodes"]))
except Exception as e:
    print(f"Mapper failed: {e}")
    raise

# K-Means Comparison
kmeans = KMeans(n_clusters=10, random_state=42)
kmeans_labels = kmeans.fit_predict(X)

# DBSCAN Comparison
dbscan = DBSCAN(eps=0.1, min_samples=5)
dbscan_labels = dbscan.fit_predict(X)

# Geospatial Visualization
map_center = [df_filtered["latitude"].median(), df_filtered["longitude"].median()]
hotspot_map = folium.Map(location=map_center, zoom_start=5, tiles='CartoDB positron')

# Plot raw data points as a heatmap
sample_size = min(5000, len(df_filtered))
df_sample = df_filtered.sample(n=sample_size, random_state=42) if len(df_filtered) > sample_size else df_filtered
heat_data = [[row['latitude'], row['longitude']] for _, row in df_sample.iterrows()]
HeatMap(heat_data, radius=10, blur=8, name="Raw Data Heatmap").add_to(hotspot_map)

# Plot Soft Mapper nodes
for node_id, indices in complex_res["nodes"].items():
    if indices:
        node_coords_orig = df_filtered.iloc[indices][['latitude', 'longitude']].values
        node_center = np.mean(node_coords_orig, axis=0)
        folium.Marker(
            location=node_center,
            popup=f"Node {node_id}: {len(indices)} points",
            icon=folium.Icon(color="red")
        ).add_to(hotspot_map)

# Plot K-Means centroids
kmeans_centers = kmeans.cluster_centers_
scaler = StandardScaler().fit(df_filtered[['latitude', 'longitude']])
kmeans_centers_orig = scaler.inverse_transform(kmeans_centers)
for centroid in kmeans_centers_orig:
    folium.CircleMarker(
        location=centroid,
        radius=5,
        color='blue',
        fill=True,
        fill_opacity=0.6,
        popup="K-Means Centroid"
    ).add_to(hotspot_map)

# Plot DBSCAN clusters
core_samples_mask = dbscan_labels != -1
core_points = X[core_samples_mask]
if len(core_points) > 0:
    core_points_orig = df_filtered.iloc[np.where(core_samples_mask)][['latitude', 'longitude']].values
    HeatMap(core_points_orig, radius=10, blur=8, name="DBSCAN Clusters").add_to(hotspot_map)

# Add layer control
folium.LayerControl().add_to(hotspot_map)

# Display map directly in Colab
hotspot_map

# Save for later viewing
hotspot_map.save("hotspot_map.html")
print("Geospatial map saved as hotspot_map.html")

# Visualization: Soft Mapper
if len(complex_res["nodes"]) > 0:
    html = mapper.visualize(
        complex_res,
        path_html="soft_mapper.html",
        title="Soft Mapper with Persistent Homology"
    )
    print("Interactive HTML saved as soft_mapper.html")
else:
    print("Skipping visualization due to empty complex")

from IPython.display import display
display(hotspot_map)